{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from label_flip_revised.utils import open_csv, create_dir\n",
    "from label_flip_revised.simple_nn_model import SimpleModel\n",
    "from label_flip_revised.torch_utils import evaluate, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukec/workspace/label_flip_revised\n"
     ]
    }
   ],
   "source": [
    "PATH_ROOT = Path(os.getcwd()).absolute().parent\n",
    "print(PATH_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/lukec/workspace/label_flip_revised/results/synth_nn/synth_nn_poison_32.csv', '/home/lukec/workspace/label_flip_revised/results/synth_nn/synth_nn_poison_43.csv', '/home/lukec/workspace/label_flip_revised/results/synth_nn/synth_nn_poison_6.csv', '/home/lukec/workspace/label_flip_revised/results/synth_nn/synth_nn_poison_31.csv', '/home/lukec/workspace/label_flip_revised/results/synth_nn/synth_nn_poison_17.csv']\n"
     ]
    }
   ],
   "source": [
    "path_cm_poison = glob(os.path.join(PATH_ROOT, 'results', 'synth_nn', '*.csv'))\n",
    "print(path_cm_poison[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2619, 36)\n"
     ]
    }
   ],
   "source": [
    "df_c_measure = pd.DataFrame()\n",
    "for p in path_cm_poison:\n",
    "    _df = pd.read_csv(p)\n",
    "    df_c_measure = pd.concat([df_c_measure, _df])\n",
    "\n",
    "print(df_c_measure.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of columns before removing NA: 35\n",
      "# of columns after removing NA: 28\n",
      "['Data', 'overlapping.F1.mean', 'overlapping.F1.sd', 'overlapping.F1v.mean', 'overlapping.F2.mean', 'overlapping.F3.mean', 'overlapping.F4.mean', 'neighborhood.N1', 'neighborhood.N2.mean', 'neighborhood.N2.sd', 'neighborhood.N3.mean', 'neighborhood.N3.sd', 'neighborhood.N4.mean', 'neighborhood.N4.sd', 'neighborhood.T1.mean', 'neighborhood.T1.sd', 'neighborhood.LSC', 'linearity.L1.mean', 'linearity.L2.mean', 'linearity.L3.mean', 'dimensionality.T2', 'dimensionality.T3', 'dimensionality.T4', 'balance.C1', 'balance.C2', 'network.Density', 'network.ClsCoef', 'network.Hubs.mean', 'network.Hubs.sd']\n"
     ]
    }
   ],
   "source": [
    "# Remove NA\n",
    "print('# of columns before removing NA:', len(df_c_measure.columns) - 1)  # Name does not count\n",
    "cols_not_na = df_c_measure.columns[df_c_measure.notna().any()].tolist()\n",
    "df_c_measure = df_c_measure[cols_not_na]\n",
    "print('# of columns after removing NA:', len(df_c_measure.columns) - 1)  # Name does not count\n",
    "print(cols_not_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns\n",
    "new_cols = [\n",
    "    'Data', 'F1', 'F1 SD', 'F1v', 'F2', 'F3', 'F4', 'N1', \n",
    "    'N2', 'N2 SD', 'N3 ', 'N3 SD', 'N4', 'N4 SD', 'T1', 'T1 SD', 'LSC', \n",
    "    'L1', 'L2', 'L3', 'T2', 'T3', 'T4', 'C1', 'C2', 'Density', 'ClsCoef', \n",
    "    'Hubs', 'HubsSD']\n",
    "new_names_map = {df_c_measure.columns[i]:new_cols[i] for i in range(len(new_cols))}\n",
    "\n",
    "df_c_measure = df_c_measure.rename(new_names_map, axis=1)\n",
    "\n",
    "df_c_measure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = [float(Path(d).stem.split('_')[-1]) for d in df_c_measure['Data'].to_list()]\n",
    "df_c_measure['rate'] = rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Running on CPU!')\n",
    "\n",
    "HIDDEN_LAYER = 128\n",
    "LR = 0.001  # Learning rate.\n",
    "MAX_EPOCHS = 400  # Number of iteration for training.\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(X, y, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    dataset = TensorDataset(\n",
    "        torch.from_numpy(X).type(torch.float32),\n",
    "        torch.from_numpy(y).type(torch.int64))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = os.path.join(PATH_ROOT, 'results')\n",
    "path_model = os.path.join(path_output, 'torch')\n",
    "create_dir(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2619/2619 [1:05:40<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "path_plot = os.path.join(path_output, f'synth_falfa_score.csv')\n",
    "\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "datanames = []\n",
    "\n",
    "if os.path.exists(path_plot):\n",
    "    df = pd.read_csv(path_plot)\n",
    "    acc_train = df['train'].to_list()\n",
    "    acc_test = df['test'].to_list()\n",
    "    datanames = df['data'].to_list()\n",
    "else:\n",
    "    postfix_clean = '_clean_test.csv'\n",
    "    postfix_torch = '_SimpleNN.torch'\n",
    "\n",
    "    for i in tqdm(range(df_c_measure.shape[0])):\n",
    "        dataname_poison = str(Path(df_c_measure.iloc[i]['Data']).stem)\n",
    "        dataname = dataname_poison[:-len('_nn_ALFA_0.20')]\n",
    "        datanames.append(dataname_poison)\n",
    "        \n",
    "        X_po, y_po, _ = open_csv(os.path.join(PATH_ROOT, 'data', 'synth', 'alfa_nn', f'{dataname_poison}.csv'))\n",
    "        X_test, y_test, _ = open_csv(os.path.join(PATH_ROOT, 'data', 'synth', 'test', f'{dataname}{postfix_clean}'))\n",
    "        n_features = X_po.shape[1]\n",
    "\n",
    "        dataloader_poison = get_dataloader(X_po, y_po, shuffle=True)\n",
    "        dataloader_test = get_dataloader(X_test, y_test, shuffle=False)\n",
    "\n",
    "        model = SimpleModel(n_features, hidden_dim=HIDDEN_LAYER, output_dim=2).to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.8)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        _path_model = os.path.join(path_model, f'{dataname_poison}{postfix_torch}')\n",
    "        if os.path.exists(_path_model):\n",
    "            model.load_state_dict(torch.load(_path_model, map_location=device))\n",
    "        else:\n",
    "            train_model(model, dataloader_poison, optimizer, loss_fn, device, MAX_EPOCHS)\n",
    "            torch.save(model.state_dict(), _path_model)\n",
    "\n",
    "        acc_po, _ = evaluate(dataloader_poison, model, loss_fn, device)\n",
    "        acc_te, _ = evaluate(dataloader_test, model, loss_fn, device)\n",
    "        acc_train.append(acc_po)\n",
    "        acc_test.append(acc_te)\n",
    "\n",
    "    results = {\n",
    "        'data': datanames,\n",
    "        'train': acc_train,\n",
    "        'test': acc_test,\n",
    "    }\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(path_plot, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize=(10, 4)\n",
    "fontsize = 14\n",
    "plt.rcParams[\"font.size\"] = fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'F1', 'F1 SD', 'F1v', 'F2', 'F3', 'F4', 'N1', \n",
    "    'N2', 'N2 SD', 'N3 ', 'N3 SD', 'N4', 'N4 SD', 'T1', 'T1 SD', 'LSC', \n",
    "    'L1', 'L2', 'L3', 'T2', 'T3', 'T4', 'C1', 'C2', 'Density', 'ClsCoef', \n",
    "    'Hubs', 'HubsSD', 'rate']\n",
    "\n",
    "df_c_measure = df_c_measure[cols]\n",
    "\n",
    "cor = df_c_measure[cols].corr()\n",
    "cor_rate = cor['rate']\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "cor_rate[cor_rate.abs().sort_values(ascending=False).keys()[1:]].plot.bar()\n",
    "plt.title('')\n",
    "plt.ylim([-1.05, 1.05])\n",
    "plt.tight_layout()\n",
    "name = 'synth_cor_rate_falfa'\n",
    "path_fig = os.path.join(path_output, f'{name}.pdf')\n",
    "plt.savefig(path_fig, dpi=300)\n",
    "\n",
    "path_output = os.path.join(PATH_ROOT, 'results', f'{name}.csv')\n",
    "cor_rate.to_csv(path_output, index=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8bcff36e631da1ef6cb25b63542a3f56d2322f7e3d69a7432caaf86f390cdb5a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('torch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
