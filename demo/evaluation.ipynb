{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from numpy.core.defchararray import find\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from prettytable import PrettyTable\n",
    "from scipy import stats\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay,\n",
    "    auc,\n",
    "    mean_squared_error,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lukec/workspace/label_flip_revised\n"
     ]
    }
   ],
   "source": [
    "PATH_ROOT = os.path.join(pathlib.Path().absolute().parent)\n",
    "print(PATH_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of different percentage tested for 1 dataset: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POISON_LVLS = np.round(np.arange(0, 0.41, 0.05, dtype=float), 2)\n",
    "\n",
    "print(\"# of different percentage tested for 1 dataset:\", len(POISON_LVLS))\n",
    "POISON_LVLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 files for C-Measures\n",
      "df_cm_poison (2619, 36)\n"
     ]
    }
   ],
   "source": [
    "path_cm = glob.glob(os.path.join(PATH_ROOT, \"results\", \"synth_nn\", \"*.csv\"))\n",
    "print(f\"Found {len(path_cm)} files for C-Measures\")\n",
    "\n",
    "df_list = []\n",
    "for p in path_cm:\n",
    "    df_list.append(pd.read_csv(p))\n",
    "df_cm_poison = pd.concat(df_list)\n",
    "print(\"df_cm_poison\", df_cm_poison.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_cm_clean (300, 36)\n"
     ]
    }
   ],
   "source": [
    "path_cm = [os.path.join(PATH_ROOT, \"results\", \"synth_svm\", f\"synth_svm_clean_{i}.csv\") for i in range(3)]\n",
    "\n",
    "df_list = []\n",
    "for p in path_cm:\n",
    "    df_list.append(pd.read_csv(p))\n",
    "df_cm_clean = pd.concat(df_list)\n",
    "print(\"df_cm_clean\", df_cm_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of columns before removing NA: 35\n",
      "# of columns after removing NA: 28\n",
      "Data,\n",
      "balance.C1,\n",
      "balance.C2,\n",
      "dimensionality.T2,\n",
      "dimensionality.T3,\n",
      "dimensionality.T4,\n",
      "linearity.L1.mean,\n",
      "linearity.L2.mean,\n",
      "linearity.L3.mean,\n",
      "neighborhood.LSC,\n",
      "neighborhood.N1,\n",
      "neighborhood.N2.mean,\n",
      "neighborhood.N2.sd,\n",
      "neighborhood.N3.mean,\n",
      "neighborhood.N3.sd,\n",
      "neighborhood.N4.mean,\n",
      "neighborhood.N4.sd,\n",
      "neighborhood.T1.mean,\n",
      "neighborhood.T1.sd,\n",
      "network.ClsCoef,\n",
      "network.Density,\n",
      "network.Hubs.mean,\n",
      "network.Hubs.sd,\n",
      "overlapping.F1.mean,\n",
      "overlapping.F1.sd,\n",
      "overlapping.F1v.mean,\n",
      "overlapping.F2.mean,\n",
      "overlapping.F3.mean,\n",
      "overlapping.F4.mean\n"
     ]
    }
   ],
   "source": [
    "# Remove NA\n",
    "# Name does not count\n",
    "print(\"# of columns before removing NA:\", len(df_cm_clean.columns) - 1)\n",
    "\n",
    "cols_not_na = df_cm_clean.columns[df_cm_clean.notna().any()].tolist()\n",
    "# Name does not count\n",
    "print(\"# of columns after removing NA:\", len(cols_not_na) - 1)\n",
    "\n",
    "df_cm_clean = df_cm_clean[cols_not_na]\n",
    "df_cm_poison = df_cm_poison[cols_not_na]\n",
    "\n",
    "print(*sorted(df_cm_clean.columns.to_list()), sep=\",\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_cm (2919, 30)\n"
     ]
    }
   ],
   "source": [
    "# Adding `Rate` column\n",
    "df_cm_clean.insert(1, \"Rate\", 0.0)\n",
    "\n",
    "rates = [float(os.path.splitext(d)[0].split(\"_\")[-1]) for d in df_cm_poison[\"Data\"].to_numpy()]\n",
    "df_cm_poison.insert(1, \"Rate\", rates)\n",
    "\n",
    "# Merge 2 dataframe together\n",
    "df_cm = pd.concat([df_cm_clean, df_cm_poison])\n",
    "print(\"df_cm\", df_cm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Data</th>\n",
       "      <th>Rate</th>\n",
       "      <th>overlapping.F1.mean</th>\n",
       "      <th>overlapping.F1.sd</th>\n",
       "      <th>overlapping.F1v.mean</th>\n",
       "      <th>overlapping.F2.mean</th>\n",
       "      <th>overlapping.F3.mean</th>\n",
       "      <th>overlapping.F4.mean</th>\n",
       "      <th>neighborhood.N1</th>\n",
       "      <th>...</th>\n",
       "      <th>linearity.L3.mean</th>\n",
       "      <th>dimensionality.T2</th>\n",
       "      <th>dimensionality.T3</th>\n",
       "      <th>dimensionality.T4</th>\n",
       "      <th>balance.C1</th>\n",
       "      <th>balance.C2</th>\n",
       "      <th>network.Density</th>\n",
       "      <th>network.ClsCoef</th>\n",
       "      <th>network.Hubs.mean</th>\n",
       "      <th>network.Hubs.sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f04_i02_r00_c01_w6_1.csv</td>\n",
       "      <td>f04_i02_r00_c01_w6_1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.810730</td>\n",
       "      <td>0.372452</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.334868</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970657</td>\n",
       "      <td>0.077664</td>\n",
       "      <td>0.848518</td>\n",
       "      <td>0.406686</td>\n",
       "      <td>0.766283</td>\n",
       "      <td>0.302880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f04_i02_r00_c01_w6_1_nn_ALFA_0.05.csv</td>\n",
       "      <td>f04_i02_r00_c01_w6_1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.871642</td>\n",
       "      <td>0.236221</td>\n",
       "      <td>0.157394</td>\n",
       "      <td>0.478438</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.6860</td>\n",
       "      <td>0.082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992482</td>\n",
       "      <td>0.020594</td>\n",
       "      <td>0.862773</td>\n",
       "      <td>0.400181</td>\n",
       "      <td>0.770124</td>\n",
       "      <td>0.299065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f04_i02_r00_c01_w6_1_nn_ALFA_0.10.csv</td>\n",
       "      <td>f04_i02_r00_c01_w6_1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.898315</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.250773</td>\n",
       "      <td>0.565290</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>0.126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880065</td>\n",
       "      <td>0.278243</td>\n",
       "      <td>0.865011</td>\n",
       "      <td>0.415929</td>\n",
       "      <td>0.642544</td>\n",
       "      <td>0.312808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f04_i02_r00_c01_w6_1_nn_ALFA_0.15.csv</td>\n",
       "      <td>f04_i02_r00_c01_w6_1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.924654</td>\n",
       "      <td>0.092116</td>\n",
       "      <td>0.302377</td>\n",
       "      <td>0.436307</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>0.115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993061</td>\n",
       "      <td>0.019025</td>\n",
       "      <td>0.870060</td>\n",
       "      <td>0.382698</td>\n",
       "      <td>0.761814</td>\n",
       "      <td>0.292958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f04_i02_r00_c01_w6_1_nn_ALFA_0.20.csv</td>\n",
       "      <td>f04_i02_r00_c01_w6_1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.932818</td>\n",
       "      <td>0.088960</td>\n",
       "      <td>0.343645</td>\n",
       "      <td>0.512272</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>0.107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.719924</td>\n",
       "      <td>0.532007</td>\n",
       "      <td>0.863499</td>\n",
       "      <td>0.437701</td>\n",
       "      <td>0.639688</td>\n",
       "      <td>0.248479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Filename                  Data  Rate  \\\n",
       "0               f04_i02_r00_c01_w6_1.csv  f04_i02_r00_c01_w6_1  0.00   \n",
       "1  f04_i02_r00_c01_w6_1_nn_ALFA_0.05.csv  f04_i02_r00_c01_w6_1  0.05   \n",
       "2  f04_i02_r00_c01_w6_1_nn_ALFA_0.10.csv  f04_i02_r00_c01_w6_1  0.10   \n",
       "3  f04_i02_r00_c01_w6_1_nn_ALFA_0.15.csv  f04_i02_r00_c01_w6_1  0.15   \n",
       "4  f04_i02_r00_c01_w6_1_nn_ALFA_0.20.csv  f04_i02_r00_c01_w6_1  0.20   \n",
       "\n",
       "   overlapping.F1.mean  overlapping.F1.sd  overlapping.F1v.mean  \\\n",
       "0             0.810730           0.372452              0.054200   \n",
       "1             0.871642           0.236221              0.157394   \n",
       "2             0.898315           0.188025              0.250773   \n",
       "3             0.924654           0.092116              0.302377   \n",
       "4             0.932818           0.088960              0.343645   \n",
       "\n",
       "   overlapping.F2.mean  overlapping.F3.mean  overlapping.F4.mean  \\\n",
       "0             0.334868                0.504               0.4245   \n",
       "1             0.478438                0.762               0.6860   \n",
       "2             0.565290                0.690               0.6360   \n",
       "3             0.436307                0.690               0.6140   \n",
       "4             0.512272                0.631               0.5850   \n",
       "\n",
       "   neighborhood.N1  ...  linearity.L3.mean  dimensionality.T2  \\\n",
       "0            0.030  ...             0.0045              0.002   \n",
       "1            0.082  ...             0.0500              0.004   \n",
       "2            0.126  ...             0.1170              0.004   \n",
       "3            0.115  ...             0.1330              0.004   \n",
       "4            0.107  ...             0.1100              0.004   \n",
       "\n",
       "   dimensionality.T3  dimensionality.T4  balance.C1  balance.C2  \\\n",
       "0              0.002                1.0    0.970657    0.077664   \n",
       "1              0.004                1.0    0.992482    0.020594   \n",
       "2              0.004                1.0    0.880065    0.278243   \n",
       "3              0.004                1.0    0.993061    0.019025   \n",
       "4              0.004                1.0    0.719924    0.532007   \n",
       "\n",
       "   network.Density  network.ClsCoef  network.Hubs.mean  network.Hubs.sd  \n",
       "0         0.848518         0.406686           0.766283         0.302880  \n",
       "1         0.862773         0.400181           0.770124         0.299065  \n",
       "2         0.865011         0.415929           0.642544         0.312808  \n",
       "3         0.870060         0.382698           0.761814         0.292958  \n",
       "4         0.863499         0.437701           0.639688         0.248479  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cm.insert(0, \"Filename\", df_cm[\"Data\"])\n",
    "\n",
    "# Update data column, the same dataset with different poison level will have consistant name\n",
    "df_cm[\"Data\"] = [\"_\".join(os.path.splitext(d)[0].split(\"_\")[:6]) for d in df_cm[\"Data\"].to_list()]\n",
    "df_cm = df_cm.sort_values([\"Data\", \"Rate\"], axis=0)\n",
    "df_cm = df_cm.reset_index(drop=True)\n",
    "\n",
    "df_cm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any row with Rate greater than 0.4\n",
    "# 45% poison rate may lead to 1 class disappear\n",
    "df_cm = df_cm[df_cm[\"Rate\"] <= 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from label_flip_revised.utils import open_csv\n",
    "from label_flip_revised import SimpleModel, train_model, evaluate, create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training the classifier:\n",
    "BATCH_SIZE = 128  # Size of mini-batch.\n",
    "HIDDEN_LAYER = 128  # Number of hidden neurons in a hidden layer.\n",
    "LR = 0.001  # Learning rate.\n",
    "MAX_EPOCHS = 300  # Number of iteration for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[f04_i02_r00_c01_w6_1] Acc train: 98.50 test: 99.40\n",
      "[f04_i02_r00_c01_w6_1_nn_ALFA_0.05.csv] Acc train: 93.60 test: 99.30\n",
      "[f04_i02_r00_c01_w6_1_nn_ALFA_0.10.csv] Acc train: 86.40 test: 94.00\n",
      "[f04_i02_r00_c01_w6_1_nn_ALFA_0.15.csv] Acc train: 87.70 test: 89.90\n",
      "[f04_i02_r00_c01_w6_1_nn_ALFA_0.20.csv] Acc train: 87.20 test: 77.20\n",
      "[f04_i02_r00_c01_w6_1_nn_ALFA_0.25.csv] Acc train: 89.40 test: 72.20\n",
      "[f04_i02_r00_c01_w6_1_nn_ALFA_0.30.csv] Acc train: 90.10 test: 60.00\n",
      "[f04_i02_r00_c01_w6_1_nn_ALFA_0.35.csv] Acc train: 91.30 test: 59.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:15<01:01, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[f04_i02_r00_c01_w6_1_nn_ALFA_0.40.csv] Acc train: 92.50 test: 60.00\n",
      "[f04_i02_r01_c01_w5_1] Acc train: 98.70 test: 99.10\n",
      "[f04_i02_r01_c01_w5_1_nn_ALFA_0.05.csv] Acc train: 93.00 test: 98.10\n",
      "[f04_i02_r01_c01_w5_1_nn_ALFA_0.10.csv] Acc train: 87.50 test: 96.70\n",
      "[f04_i02_r01_c01_w5_1_nn_ALFA_0.15.csv] Acc train: 85.90 test: 90.90\n",
      "[f04_i02_r01_c01_w5_1_nn_ALFA_0.20.csv] Acc train: 84.70 test: 85.40\n",
      "[f04_i02_r01_c01_w5_1_nn_ALFA_0.25.csv] Acc train: 81.20 test: 60.50\n",
      "[f04_i02_r01_c01_w5_1_nn_ALFA_0.30.csv] Acc train: 78.40 test: 50.00\n",
      "[f04_i02_r01_c01_w5_1_nn_ALFA_0.35.csv] Acc train: 91.70 test: 57.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:28<00:42, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[f04_i02_r01_c01_w5_1_nn_ALFA_0.40.csv] Acc train: 86.80 test: 50.00\n",
      "[f04_i03_r00_c02_w5_1] Acc train: 89.90 test: 92.40\n",
      "[f04_i03_r00_c02_w5_1_nn_ALFA_0.05.csv] Acc train: 82.30 test: 90.80\n",
      "[f04_i03_r00_c02_w5_1_nn_ALFA_0.10.csv] Acc train: 80.50 test: 88.60\n",
      "[f04_i03_r00_c02_w5_1_nn_ALFA_0.15.csv] Acc train: 78.50 test: 73.60\n",
      "[f04_i03_r00_c02_w5_1_nn_ALFA_0.20.csv] Acc train: 79.30 test: 72.10\n",
      "[f04_i03_r00_c02_w5_1_nn_ALFA_0.25.csv] Acc train: 75.00 test: 50.00\n",
      "[f04_i03_r00_c02_w5_1_nn_ALFA_0.30.csv] Acc train: 89.00 test: 61.20\n",
      "[f04_i03_r00_c02_w5_1_nn_ALFA_0.35.csv] Acc train: 85.00 test: 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:42<00:27, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[f04_i03_r00_c02_w5_1_nn_ALFA_0.40.csv] Acc train: 91.90 test: 51.30\n",
      "[f04_i03_r01_c01_w6_1] Acc train: 97.80 test: 97.10\n",
      "[f04_i03_r01_c01_w6_1_nn_ALFA_0.05.csv] Acc train: 91.50 test: 96.00\n",
      "[f04_i03_r01_c01_w6_1_nn_ALFA_0.10.csv] Acc train: 91.00 test: 92.60\n",
      "[f04_i03_r01_c01_w6_1_nn_ALFA_0.15.csv] Acc train: 90.90 test: 81.40\n",
      "[f04_i03_r01_c01_w6_1_nn_ALFA_0.20.csv] Acc train: 92.30 test: 79.10\n",
      "[f04_i03_r01_c01_w6_1_nn_ALFA_0.25.csv] Acc train: 84.70 test: 59.60\n",
      "[f04_i03_r01_c01_w6_1_nn_ALFA_0.30.csv] Acc train: 83.70 test: 63.70\n",
      "[f04_i03_r01_c01_w6_1_nn_ALFA_0.35.csv] Acc train: 94.70 test: 59.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:55<00:13, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[f04_i03_r01_c01_w6_1_nn_ALFA_0.40.csv] Acc train: 94.10 test: 54.90\n",
      "[f05_i03_r02_c02_w5_1] Acc train: 91.40 test: 91.10\n",
      "[f05_i03_r02_c02_w5_1_nn_ALFA_0.05.csv] Acc train: 85.50 test: 89.90\n",
      "[f05_i03_r02_c02_w5_1_nn_ALFA_0.10.csv] Acc train: 80.30 test: 89.30\n",
      "[f05_i03_r02_c02_w5_1_nn_ALFA_0.15.csv] Acc train: 85.50 test: 72.70\n",
      "[f05_i03_r02_c02_w5_1_nn_ALFA_0.20.csv] Acc train: 77.40 test: 61.70\n",
      "[f05_i03_r02_c02_w5_1_nn_ALFA_0.25.csv] Acc train: 84.20 test: 65.10\n",
      "[f05_i03_r02_c02_w5_1_nn_ALFA_0.30.csv] Acc train: 79.90 test: 49.90\n",
      "[f05_i03_r02_c02_w5_1_nn_ALFA_0.35.csv] Acc train: 88.90 test: 57.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:10<00:00, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[f05_i03_r02_c02_w5_1_nn_ALFA_0.40.csv] Acc train: 89.90 test: 49.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute Train acc and test acc\n",
    "datanames = df_cm['Data'].unique()\n",
    "for dataname in tqdm(datanames[:5]):\n",
    "    df_subset = df_cm[df_cm[\"Data\"] == dataname]\n",
    "\n",
    "    # Only 1 clean file, the rest of data are poisoned\n",
    "    # TODO: The C-Measure is computed from the entire dataset instead of training set.\n",
    "    # TODO: Recompute C-Measure for the clean training sets!\n",
    "    # filename_clean = df_subset[df_subset[\"Rate\"] == 0].at[0, \"Filename\"]\n",
    "    filelist_poison = df_subset[df_subset[\"Rate\"] != 0]['Filename'].to_list()\n",
    "\n",
    "    # Load clean data\n",
    "    X_train, y_train, _ = open_csv(os.path.join(PATH_ROOT, \"data\", \"synth\", \"train\", f\"{dataname}_clean_train.csv\"))\n",
    "    X_test, y_test, _ = open_csv(os.path.join(PATH_ROOT, \"data\", \"synth\", \"test\", f\"{dataname}_clean_test.csv\"))\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\n",
    "    n_features = X_train.shape[1]\n",
    "    model_clean = SimpleModel(n_features, hidden_dim=HIDDEN_LAYER, output_dim=2).to(device)\n",
    "    path_model = os.path.join(PATH_ROOT, \"data\", \"synth\", \"torch\", f\"{dataname}_SimpleNN.torch\")\n",
    "    model_clean.load_state_dict(torch.load(path_model, map_location=device))\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataset = TensorDataset(torch.from_numpy(X_train).type(torch.float32),\n",
    "                            torch.from_numpy(y_train).type(torch.int64))\n",
    "    dataloader_train = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    dataset = TensorDataset(torch.from_numpy(X_test).type(torch.float32),\n",
    "                            torch.from_numpy(y_test).type(torch.int64))\n",
    "    dataloader_test = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    acc_train, _ = evaluate(dataloader_train, model_clean, loss_fn, device)\n",
    "    acc_test, _ = evaluate(dataloader_test, model_clean, loss_fn, device)\n",
    "    print(f\"[{dataname}] Acc train: {acc_train*100:.2f} test: {acc_test*100:.2f}\")\n",
    "    # TODO: Save results into the dataframe\n",
    "\n",
    "    for idx in df_subset[df_subset[\"Rate\"] != 0].index:\n",
    "        data_poison = df_subset.at[idx, \"Filename\"]\n",
    "        path_data = os.path.join(PATH_ROOT, \"data\", \"synth\", \"alfa_nn\", data_poison)\n",
    "        X_poison, y_poison, _ = open_csv(path_data)\n",
    "        np.testing.assert_array_almost_equal(X_poison, X_train)\n",
    "        assert not np.array_equal(y_poison, y_train)\n",
    "        dataset = TensorDataset(torch.from_numpy(X_train).type(torch.float32),\n",
    "                                torch.from_numpy(y_poison).type(torch.int64))\n",
    "        dataloader_poison = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        model_poison = SimpleModel(n_features, hidden_dim=HIDDEN_LAYER, output_dim=2).to(device)\n",
    "        optimizer = torch.optim.SGD(model_poison.parameters(), lr=LR, momentum=0.8)\n",
    "        train_model(model_poison,\n",
    "                    dataloader_poison,\n",
    "                    optimizer=optimizer,\n",
    "                    loss_fn=loss_fn,\n",
    "                    device=device,\n",
    "                    max_epochs=MAX_EPOCHS)\n",
    "        # TODO: Load pre-trained model, if it already exists.\n",
    "        # Save model\n",
    "        path_poison = os.path.join(PATH_ROOT, \"data\", \"synth\", \"torch\", \"poison\")\n",
    "        create_dir(path_poison)\n",
    "        path_model = os.path.join(path_poison, os.path.splitext(data_poison)[0] + \".torch\")\n",
    "        torch.save(model_poison.state_dict(), path_model)\n",
    "\n",
    "        acc_poison, _ = evaluate(dataloader_poison, model_poison, loss_fn, device)\n",
    "        acc_test, _ = evaluate(dataloader_test, model_poison, loss_fn, device)\n",
    "        print(f\"[{data_poison}] Acc train: {acc_poison*100:.2f} test: {acc_test*100:.2f}\")\n",
    "        # TODO: Save results into the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 3 difficulty levels: Hard, Normal Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal hyperparameters for regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve (All difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve (Group by difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "016355130b3c16c526f1441741bcbcb9475435ab9822383558c43dece6aac7b7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('venv37': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
